{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Smooth Quantile Regression with QuantileHuber\n\nThis example compares sklearn's standard quantile regression with skglm's smooth\napproximation. Skglm's quantile regression uses a smooth Huber-like approximation\n(quadratic near zero, linear in the tails) to replace the non-differentiable\npinball loss. Progressive smoothing enables efficient gradient-based optimization,\nmaintaining speed and accuracy also on large-scale, high-dimensional datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Florian Kozikowski\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import QuantileRegressor\nfrom skglm.experimental.quantile_huber import QuantileHuber, SmoothQuantileRegressor\n\n# Generate regression data\nX, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=0)\ntau = 0.8  # 80th percentile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare standard vs smooth quantile regression\nBoth methods solve the same problem but with different loss functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Standard quantile regression (sklearn)\nstart = time.time()\nsk_model = QuantileRegressor(quantile=tau, alpha=0.1)\nsk_model.fit(X, y)\nsk_time = time.time() - start\n\n# Smooth quantile regression (skglm)\nstart = time.time()\nsmooth_model = SmoothQuantileRegressor(\n    quantile=tau,\n    alpha=0.1,\n    delta_init=0.5,      # Initial smoothing parameter\n    delta_final=0.01,    # Final smoothing (smaller = closer to true quantile)\n    n_deltas=5           # Number of continuation steps\n)\nsmooth_model.fit(X, y)\nsmooth_time = time.time() - start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate both methods\nCoverage: fraction of true values below predictions (should \u2248 tau)\nPinball loss: standard quantile regression evaluation metric\n\nNote: No robust benchmarking conducted yet. The speed advantagous likely only\nshows on large-scale, high-dimensional datasets. The sklearn implementation is\nlikely faster on small datasets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pinball_loss(residuals, quantile):\n    return np.mean(residuals * (quantile - (residuals < 0)))\n\n\nsk_pred = sk_model.predict(X)\nsmooth_pred = smooth_model.predict(X)\n\nprint(f\"{'Method':<15} {'Coverage':<10} {'Time (s)':<10} {'Pinball Loss':<12}\")\nprint(\"-\" * 50)\nprint(f\"{'Sklearn':<15} {np.mean(y <= sk_pred):<10.3f} {sk_time:<10.3f} \"\n      f\"{pinball_loss(y - sk_pred, tau):<12.4f}\")\nprint(f\"{'SmoothQuantile':<15} {np.mean(y <= smooth_pred):<10.3f} {smooth_time:<10.3f} \"\n      f\"{pinball_loss(y - smooth_pred, tau):<12.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the smooth approximation\nThe smooth loss approximates the pinball loss but with continuous gradients\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Show loss and gradient for different quantile levels\nresiduals = np.linspace(-3, 3, 500)\ndelta = 0.5\nquantiles = [0.1, 0.5, 0.9]\n\nfor tau_val in quantiles:\n    qh = QuantileHuber(quantile=tau_val, delta=delta)\n    loss = [qh._loss_sample(r) for r in residuals]\n    grad = [qh._grad_per_sample(r) for r in residuals]\n\n    # Compute pinball loss for each residual\n    pinball_loss = [r * (tau_val - (r < 0)) for r in residuals]\n\n    # Plot smooth loss and pinball loss\n    ax1.plot(residuals, loss, label=f\"\u03c4={tau_val}\", linewidth=2)\n    ax1.plot(residuals, pinball_loss, '--', alpha=0.4, color='gray',\n             label=f\"Pinball \u03c4={tau_val}\")\n    ax2.plot(residuals, grad, label=f\"\u03c4={tau_val}\", linewidth=2)\n\n# Add vertical lines and shading showing delta boundaries\nfor ax in [ax1, ax2]:\n    ax.axvline(-delta, color='gray', linestyle='--', alpha=0.7, linewidth=1.5)\n    ax.axvline(delta, color='gray', linestyle='--', alpha=0.7, linewidth=1.5)\n    # Add shading for quadratic region\n    ax.axvspan(-delta, delta, alpha=0.15, color='gray')\n\n# Add delta labels\nax1.text(-delta, 0.1, '\u2212\u03b4', ha='right', va='bottom', color='gray', fontsize=10)\nax1.text(delta, 0.1, '+\u03b4', ha='left', va='bottom', color='gray', fontsize=10)\n\nax1.set_title(f\"Smooth Quantile Loss (\u03b4={delta})\", fontsize=12)\nax1.set_xlabel(\"Residual\")\nax1.set_ylabel(\"Loss\")\nax1.legend(loc='upper left')\nax1.grid(True, alpha=0.3)\n\nax2.set_title(\"Gradient (continuous everywhere)\", fontsize=12)\nax2.set_xlabel(\"Residual\")\nax2.set_ylabel(\"Gradient\")\nax2.legend(loc='upper left')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The left plot shows the asymmetric loss: tau=0.1 penalizes overestimation more,\nwhile tau=0.9 penalizes underestimation. As delta decreases towards zero, the\nloss function approaches the standard pinball loss.\nThe right plot reveals the key advantage: gradients transition smoothly through\nzero, unlike standard quantile regression which has a kink. This smoothing\nenables fast convergence with gradient-based solvers.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}