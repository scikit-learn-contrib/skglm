{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sparse recovery with non-convex penalties\nIllustrate the superior performance of penalties for sparse recovery.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Mathurin Massias\n#         Quentin Bertrand\n#         Quentin Klopfenstein\n\nimport numpy as np\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, mean_squared_error\n\nfrom skglm.utils.data import make_correlated_data\nfrom skglm.solvers import AndersonCD\nfrom skglm.datafits import Quadratic\nfrom skglm.utils.jit_compilation import compiled_clone\nfrom skglm.penalties import L1, MCPenalty, L0_5, L2_3, SCAD\n\ncmap = plt.get_cmap('tab10')\n\n# Simulate sparse data\nn_features = 1000\ndensity = 0.1\nnp.random.seed(0)\nsupp = np.random.choice(n_features, size=int(density * n_features),\n                        replace=False)\nw_true = np.zeros(n_features)\nw_true[supp] = 1\nX_, y_, w_true = make_correlated_data(\n    n_samples=1000, n_features=1000, snr=5, random_state=2,\n    rho=0.5, w_true=w_true)\n\n# standardize for MCP\nX_ /= norm(X_, axis=0) / np.sqrt(len(X_))\nX, X_test, y, y_test = train_test_split(X_, y_, test_size=0.5)\n\n\n# Compute l1 penalty value which leads to 0 as solution\nalpha_max = norm(X.T @ y, ord=np.inf) / len(y)\n\n# Define a range of penalty values\nn_alphas = 30\nalphas = alpha_max * np.geomspace(1, 1e-2, num=n_alphas)\n\ndatafit = Quadratic()\n\npenalties = {}\npenalties['lasso'] = L1(alpha=1)\npenalties['mcp'] = MCPenalty(alpha=1, gamma=3)\npenalties['scad'] = SCAD(alpha=1, gamma=3)\npenalties['l05'] = L0_5(alpha=1)\npenalties['l23'] = L2_3(alpha=1)\n\ncolors = {}\ncolors['lasso'] = cmap(0)\ncolors['mcp'] = cmap(1)\ncolors['scad'] = cmap(2)\ncolors['l05'] = cmap(3)\ncolors['l23'] = cmap(4)\n\nf1 = {}\nestimation_error = {}\nprediction_error = {}\nl0 = {}\nmse_ref = mean_squared_error(np.zeros_like(y_test), y_test)\n\nsolver = AndersonCD(ws_strategy=\"fixpoint\", fit_intercept=False)\n\nfor idx, estimator in enumerate(penalties.keys()):\n    print(f'Running {estimator}...')\n    estimator_path = solver.path(\n        X, y, compiled_clone(datafit), compiled_clone(penalties[estimator]),\n        alphas=alphas)\n\n    f1_temp = np.zeros(n_alphas)\n    prediction_error_temp = np.zeros(n_alphas)\n\n    for j, w in enumerate(estimator_path[1].T):\n        f1_temp[j] = f1_score(w != 0, w_true != 0)\n        prediction_error_temp[j] = mean_squared_error(X_test @ w, y_test) / mse_ref\n\n    f1[estimator] = f1_temp\n    prediction_error[estimator] = prediction_error_temp\n\nname_estimators = {'lasso': \"Lasso\"}\nname_estimators['mcp'] = r\"MCP, $\\gamma=%s$\" % 3\nname_estimators['scad'] = r\"SCAD, $\\gamma=%s$\" % 3\nname_estimators['l05'] = r\"$\\ell_{1/2}$\"\nname_estimators['l23'] = r\"$\\ell_{2/3}$\"\n\n\nplt.close('all')\nfig, axarr = plt.subplots(2, 1, sharex=True, sharey=False, figsize=[\n                          6.3, 4], constrained_layout=True)\n\nfor idx, estimator in enumerate(penalties.keys()):\n\n    axarr[0].semilogx(\n        alphas / alphas[0], f1[estimator], label=name_estimators[estimator],\n        c=colors[estimator])\n\n    axarr[1].semilogx(\n        alphas / alphas[0], prediction_error[estimator],\n        label=name_estimators[estimator], c=colors[estimator])\n\n    max_f1 = np.argmax(f1[estimator])\n    axarr[0].vlines(\n        x=alphas[max_f1] / alphas[0], ymin=0,\n        ymax=np.max(f1[estimator]),\n        color=colors[estimator], linestyle='--')\n    line1 = axarr[0].plot(\n        [alphas[max_f1] / alphas[0]], 0, clip_on=False,\n        marker='X', color=colors[estimator], markersize=12)\n\n    min_error = np.argmin(prediction_error[estimator])\n\n    lims = axarr[1].get_ylim()\n    axarr[1].vlines(\n        x=alphas[min_error] / alphas[0], ymin=0,\n        ymax=np.min(prediction_error[estimator]),\n        color=colors[estimator], linestyle='--')\n\n    line2 = axarr[1].plot(\n        [alphas[min_error] / alphas[0]], 0, clip_on=False,\n        marker='X', color=colors[estimator], markersize=12)\n    axarr[1].set_xlabel(r\"$\\lambda / \\lambda_{\\mathrm{max}}$\")\n    axarr[0].set_ylabel(\"F1-score\")\n    axarr[0].set_ylim(ymin=0, ymax=1.0)\n    axarr[1].set_ylim(ymin=0, ymax=lims[1])\n    axarr[1].set_ylabel(\"pred. RMSE left-out\")\n    axarr[0].legend(\n        bbox_to_anchor=(0, 1.02, 1, 0.2), loc=\"lower left\",\n        mode=\"expand\", borderaxespad=0, ncol=5)\n\nplt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}