{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Show U-curve of regularization\nIllustrate the sweet spot of regularization: not too much, not too little.\nWe showcase that for the Lasso estimator on the ``rcv1.binary`` dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\nfrom libsvmdata import fetch_libsvm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom skglm import Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the dataset and keep 2000 features.\nWe also retrain 2000 samples in training dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = fetch_libsvm(\"rcv1.binary\")\n\nX = X[:, :2000]\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nX_train, y_train = X_train[:2000], y_train[:2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the regularization path.\nFor Lasso, it is well know that there is an ``alpha_max`` above which the optimal solution is the zero vector.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha_max = norm(X_train.T @ y_train, ord=np.inf) / len(y_train)\nalphas = alpha_max * np.geomspace(1, 1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's train the estimator along the regularization path and then compute the MSE on train and test data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mse_train = []\nmse_test = []\n\nclf = Lasso(fit_intercept=False, tol=1e-8, warm_start=True)\nfor idx, alpha in enumerate(alphas):\n    clf.alpha = alpha\n    clf.fit(X_train, y_train)\n\n    mse_train.append(mean_squared_error(y_train, clf.predict(X_train)))\n    mse_test.append(mean_squared_error(y_test, clf.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can plot the train and test MSE.\nNotice the \"sweet spot\" at around ``1e-4``, which sits at the boundary between underfitting and overfitting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.close('all')\nplt.semilogx(alphas, mse_train, label='train MSE')\nplt.semilogx(alphas, mse_test, label='test MSE')\nplt.legend()\nplt.title(\"Mean squared error\")\nplt.xlabel(r\"Lasso regularization strength $\\lambda$\")\nplt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}