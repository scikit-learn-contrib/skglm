
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_generalized_linear_estimator_cv.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_generalized_linear_estimator_cv.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_generalized_linear_estimator_cv.py:


===================================
Cross-Validation for Generalized Linear Models
===================================

This example shows how to use cross-validation to automatically select
the optimal regularization parameter for generalized linear models.

.. GENERATED FROM PYTHON SOURCE LINES 9-22

.. code-block:: Python


    # Author: Florian Kozikowski

    import numpy as np
    import matplotlib.pyplot as plt

    from skglm.utils.data import make_correlated_data
    from skglm.cv import GeneralizedLinearEstimatorCV
    from skglm.estimators import GeneralizedLinearEstimator
    from skglm.datafits import Quadratic
    from skglm.penalties import L1_plus_L2
    from skglm.solvers import AndersonCD








.. GENERATED FROM PYTHON SOURCE LINES 23-25

Generate correlated data with sparse ground truth
--------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 25-29

.. code-block:: Python

    X, y, true_coef = make_correlated_data(
        n_samples=150, n_features=300, random_state=42
    )








.. GENERATED FROM PYTHON SOURCE LINES 30-33

Fit model using cross-validation
--------------------------------
The CV estimator automatically finds the best regularization strength

.. GENERATED FROM PYTHON SOURCE LINES 33-47

.. code-block:: Python

    estimator = GeneralizedLinearEstimatorCV(
        datafit=Quadratic(),
        penalty=L1_plus_L2(alpha=1.0, l1_ratio=0.5),
        solver=AndersonCD(max_iter=100),
        cv=5,
        n_alphas=50,
    )
    estimator.fit(X, y)

    print(f"Best alpha: {estimator.alpha_:.3f}")
    n_nonzero = np.sum(estimator.coef_ != 0)
    n_true_nonzero = np.sum(true_coef != 0)
    print(f"Non-zero coefficients: {n_nonzero} (true: {n_true_nonzero})")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Best alpha: 0.250
    Non-zero coefficients: 72 (true: 60)




.. GENERATED FROM PYTHON SOURCE LINES 48-51

Visualize the cross-validation path
-----------------------------------
Plot shows how CV balances model complexity with prediction performance

.. GENERATED FROM PYTHON SOURCE LINES 51-96

.. code-block:: Python


    # Get mean CV scores
    mean_scores = np.mean(estimator.scores_path_, axis=1)
    std_scores = np.std(estimator.scores_path_, axis=1)
    best_idx = np.argmax(mean_scores)
    best_alpha = estimator.alphas_[best_idx]

    # Compute coefficient paths
    coef_paths = []
    for alpha in estimator.alphas_:
        est_temp = GeneralizedLinearEstimator(
            datafit=Quadratic(),
            penalty=L1_plus_L2(alpha=alpha, l1_ratio=0.5),
            solver=AndersonCD(max_iter=100)
        )
        est_temp.fit(X, y)
        coef_paths.append(est_temp.coef_)
    coef_paths = np.array(coef_paths)

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10), sharex=True)

    ax1.semilogx(estimator.alphas_, -mean_scores, 'b-', linewidth=2, label='MSE')
    ax1.fill_between(estimator.alphas_,
                     -mean_scores - std_scores,
                     -mean_scores + std_scores,
                     alpha=0.2, label='Â±1 std. dev.')
    ax1.axvline(best_alpha, color='red', linestyle='--',
                label=f'Best alpha = {best_alpha:.2e}')
    ax1.set_ylabel('MSE')
    ax1.set_title('Cross-Validation Score vs. Regularization')
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)
    ax1.set_xlabel('alpha')

    for j in range(coef_paths.shape[1]):
        ax2.semilogx(estimator.alphas_, coef_paths[:, j], lw=1, alpha=0.3)
    ax2.axvline(best_alpha, color='red', linestyle='--')
    ax2.set_xlabel('alpha')
    ax2.set_ylabel('Coefficient value')
    ax2.set_title('Regularization Path of Coefficients')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()




.. image-sg:: /auto_examples/images/sphx_glr_plot_generalized_linear_estimator_cv_001.png
   :alt: Cross-Validation Score vs. Regularization, Regularization Path of Coefficients
   :srcset: /auto_examples/images/sphx_glr_plot_generalized_linear_estimator_cv_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 97-102

Top panel: Mean CV MSE shows U-shape, minimized at chosen alpha for optimal
bias-variance tradeoff.

Bottom panel: At this alpha, most coefficients are shrunk (many near zero),
highlighting a sparse subset of key predictors.

.. GENERATED FROM PYTHON SOURCE LINES 105-108

Visualize distance to true coefficients
----------------------------------------
Compute how well different regularization strengths recover the true coefficients

.. GENERATED FROM PYTHON SOURCE LINES 108-134

.. code-block:: Python


    distances = []
    for alpha in estimator.alphas_:
        est_temp = GeneralizedLinearEstimator(
            datafit=Quadratic(),
            penalty=L1_plus_L2(alpha=alpha, l1_ratio=0.5),
            solver=AndersonCD(max_iter=100)
        )
        est_temp.fit(X, y)
        distances.append(np.linalg.norm(est_temp.coef_ - true_coef, ord=1))

    plt.figure(figsize=(8, 5))
    plt.loglog(estimator.alphas_, distances, 'b-', linewidth=2)
    plt.axvline(estimator.alpha_, color='red', linestyle='--',
                label=f'CV-selected alpha = {estimator.alpha_:.3f}')
    plt.xlabel('Alpha (regularization strength)')
    plt.ylabel('L1 distance to true coefficients')
    plt.title('Recovery of True Coefficients')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    print(
        f"Distance at CV-selected alpha: "
        f"{np.linalg.norm(estimator.coef_ - true_coef, ord=1):.3f}")




.. image-sg:: /auto_examples/images/sphx_glr_plot_generalized_linear_estimator_cv_002.png
   :alt: Recovery of True Coefficients
   :srcset: /auto_examples/images/sphx_glr_plot_generalized_linear_estimator_cv_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Distance at CV-selected alpha: 40.030




.. GENERATED FROM PYTHON SOURCE LINES 135-139

The U-shaped curve shows two failure modes: small alpha doesn't induce
enough sparsity (keeping noisy/irrelevant features), while large alpha
overshrinks all coefficients including the true signals. Cross-validation
finds a good balance without needing access to the ground truth.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 9.716 seconds)


.. _sphx_glr_download_auto_examples_plot_generalized_linear_estimator_cv.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_generalized_linear_estimator_cv.ipynb <plot_generalized_linear_estimator_cv.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_generalized_linear_estimator_cv.py <plot_generalized_linear_estimator_cv.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_generalized_linear_estimator_cv.zip <plot_generalized_linear_estimator_cv.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
