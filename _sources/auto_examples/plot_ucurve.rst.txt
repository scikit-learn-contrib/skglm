
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_ucurve.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_ucurve.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_ucurve.py:


==============================
Show U-curve of regularization
==============================
Illustrate the sweet spot of regularization: not too much, not too little.
We showcase that for the Lasso estimator on the ``rcv1.binary`` dataset.

.. GENERATED FROM PYTHON SOURCE LINES 8-19

.. code-block:: Python


    import numpy as np
    from numpy.linalg import norm
    import matplotlib.pyplot as plt
    from libsvmdata import fetch_libsvm

    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error

    from skglm import Lasso








.. GENERATED FROM PYTHON SOURCE LINES 20-22

First, we load the dataset and keep 2000 features.
We also retrain 2000 samples in training dataset.

.. GENERATED FROM PYTHON SOURCE LINES 22-28

.. code-block:: Python

    X, y = fetch_libsvm("rcv1.binary")

    X = X[:, :2000]
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    X_train, y_train = X_train[:2000], y_train[:2000]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    file_sizes:   0%|                                   | 0.00/13.7M [00:00<?, ?B/s]    file_sizes:   0%|                           | 24.6k/13.7M [00:00<01:47, 128kB/s]    file_sizes:   0%|                           | 49.2k/13.7M [00:00<01:47, 127kB/s]    file_sizes:   1%|▏                           | 106k/13.7M [00:00<01:06, 204kB/s]    file_sizes:   2%|▍                           | 221k/13.7M [00:00<00:37, 357kB/s]    file_sizes:   3%|▉                           | 451k/13.7M [00:00<00:20, 654kB/s]    file_sizes:   7%|█▊                         | 909k/13.7M [00:01<00:10, 1.23MB/s]    file_sizes:  13%|███▍                      | 1.83M/13.7M [00:01<00:05, 2.38MB/s]    file_sizes:  27%|██████▉                   | 3.66M/13.7M [00:01<00:02, 4.54MB/s]    file_sizes:  38%|█████████▉                | 5.23M/13.7M [00:02<00:02, 3.49MB/s]    file_sizes:  50%|████████████▉             | 6.81M/13.7M [00:02<00:01, 4.47MB/s]    file_sizes:  61%|███████████████▊          | 8.38M/13.7M [00:02<00:01, 5.29MB/s]    file_sizes:  67%|█████████████████▎        | 9.17M/13.7M [00:02<00:00, 5.00MB/s]    file_sizes:  78%|████████████████████▎     | 10.7M/13.7M [00:02<00:00, 5.78MB/s]    file_sizes:  84%|█████████████████████▊    | 11.5M/13.7M [00:03<00:00, 5.33MB/s]    file_sizes:  90%|███████████████████████▎  | 12.3M/13.7M [00:03<00:00, 4.99MB/s]    file_sizes: 100%|██████████████████████████| 13.7M/13.7M [00:03<00:00, 5.64MB/s]    file_sizes: 100%|██████████████████████████| 13.7M/13.7M [00:03<00:00, 3.91MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 29-31

Next, we define the regularization path.
For Lasso, it is well know that there is an ``alpha_max`` above which the optimal solution is the zero vector.

.. GENERATED FROM PYTHON SOURCE LINES 31-34

.. code-block:: Python

    alpha_max = norm(X_train.T @ y_train, ord=np.inf) / len(y_train)
    alphas = alpha_max * np.geomspace(1, 1e-4)








.. GENERATED FROM PYTHON SOURCE LINES 35-36

Let's train the estimator along the regularization path and then compute the MSE on train and test data.

.. GENERATED FROM PYTHON SOURCE LINES 36-47

.. code-block:: Python

    mse_train = []
    mse_test = []

    clf = Lasso(fit_intercept=False, tol=1e-8, warm_start=True)
    for idx, alpha in enumerate(alphas):
        clf.alpha = alpha
        clf.fit(X_train, y_train)

        mse_train.append(mean_squared_error(y_train, clf.predict(X_train)))
        mse_test.append(mean_squared_error(y_test, clf.predict(X_test)))








.. GENERATED FROM PYTHON SOURCE LINES 48-50

Finally, we can plot the train and test MSE.
Notice the "sweet spot" at around ``1e-4``, which sits at the boundary between underfitting and overfitting.

.. GENERATED FROM PYTHON SOURCE LINES 50-57

.. code-block:: Python

    plt.close('all')
    plt.semilogx(alphas, mse_train, label='train MSE')
    plt.semilogx(alphas, mse_test, label='test MSE')
    plt.legend()
    plt.title("Mean squared error")
    plt.xlabel(r"Lasso regularization strength $\lambda$")
    plt.show(block=False)



.. image-sg:: /auto_examples/images/sphx_glr_plot_ucurve_001.png
   :alt: Mean squared error
   :srcset: /auto_examples/images/sphx_glr_plot_ucurve_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 15.141 seconds)


.. _sphx_glr_download_auto_examples_plot_ucurve.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_ucurve.ipynb <plot_ucurve.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_ucurve.py <plot_ucurve.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_ucurve.zip <plot_ucurve.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
