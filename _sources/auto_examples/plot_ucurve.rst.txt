
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_ucurve.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_ucurve.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_ucurve.py:


==============================
Show U-curve of regularization
==============================
Illustrate the sweet spot of regularization: not too much, not too little.
We showcase that for the Lasso estimator on the ``rcv1.binary`` dataset.

.. GENERATED FROM PYTHON SOURCE LINES 8-19

.. code-block:: Python


    import numpy as np
    from numpy.linalg import norm
    import matplotlib.pyplot as plt
    from libsvmdata import fetch_libsvm

    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error

    from skglm import Lasso








.. GENERATED FROM PYTHON SOURCE LINES 20-22

First, we load the dataset and keep 2000 features.
We also retrain 2000 samples in training dataset.

.. GENERATED FROM PYTHON SOURCE LINES 22-28

.. code-block:: Python

    X, y = fetch_libsvm("rcv1.binary")

    X = X[:, :2000]
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    X_train, y_train = X_train[:2000], y_train[:2000]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    file_sizes:   0%|                                   | 0.00/13.7M [00:00<?, ?B/s]    file_sizes:   0%|                           | 24.6k/13.7M [00:00<01:55, 119kB/s]    file_sizes:   0%|                           | 49.2k/13.7M [00:00<01:55, 119kB/s]    file_sizes:   1%|▏                           | 106k/13.7M [00:00<01:11, 191kB/s]    file_sizes:   1%|▎                           | 156k/13.7M [00:00<01:04, 210kB/s]    file_sizes:   2%|▌                           | 270k/13.7M [00:01<00:40, 334kB/s]    file_sizes:   6%|█▍                         | 762k/13.7M [00:01<00:12, 1.03MB/s]    file_sizes:   8%|██▏                       | 1.16M/13.7M [00:01<00:09, 1.32MB/s]    file_sizes:  23%|█████▉                    | 3.12M/13.7M [00:01<00:02, 3.88MB/s]    file_sizes:  34%|████████▉                 | 4.69M/13.7M [00:01<00:01, 5.03MB/s]    file_sizes:  46%|███████████▊              | 6.27M/13.7M [00:02<00:01, 5.83MB/s]    file_sizes:  57%|██████████████▊           | 7.84M/13.7M [00:02<00:00, 6.38MB/s]    file_sizes:  69%|█████████████████▊        | 9.41M/13.7M [00:02<00:00, 6.77MB/s]    file_sizes:  80%|████████████████████▊     | 11.0M/13.7M [00:02<00:00, 7.05MB/s]    file_sizes:  99%|█████████████████████████▊| 13.6M/13.7M [00:02<00:00, 8.69MB/s]    file_sizes: 100%|██████████████████████████| 13.7M/13.7M [00:02<00:00, 4.73MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 29-31

Next, we define the regularization path.
For Lasso, it is well know that there is an ``alpha_max`` above which the optimal solution is the zero vector.

.. GENERATED FROM PYTHON SOURCE LINES 31-34

.. code-block:: Python

    alpha_max = norm(X_train.T @ y_train, ord=np.inf) / len(y_train)
    alphas = alpha_max * np.geomspace(1, 1e-4)








.. GENERATED FROM PYTHON SOURCE LINES 35-36

Let's train the estimator along the regularization path and then compute the MSE on train and test data.

.. GENERATED FROM PYTHON SOURCE LINES 36-47

.. code-block:: Python

    mse_train = []
    mse_test = []

    clf = Lasso(fit_intercept=False, tol=1e-8, warm_start=True)
    for idx, alpha in enumerate(alphas):
        clf.alpha = alpha
        clf.fit(X_train, y_train)

        mse_train.append(mean_squared_error(y_train, clf.predict(X_train)))
        mse_test.append(mean_squared_error(y_test, clf.predict(X_test)))








.. GENERATED FROM PYTHON SOURCE LINES 48-50

Finally, we can plot the train and test MSE.
Notice the "sweet spot" at around ``1e-4``, which sits at the boundary between underfitting and overfitting.

.. GENERATED FROM PYTHON SOURCE LINES 50-57

.. code-block:: Python

    plt.close('all')
    plt.semilogx(alphas, mse_train, label='train MSE')
    plt.semilogx(alphas, mse_test, label='test MSE')
    plt.legend()
    plt.title("Mean squared error")
    plt.xlabel(r"Lasso regularization strength $\lambda$")
    plt.show(block=False)



.. image-sg:: /auto_examples/images/sphx_glr_plot_ucurve_001.png
   :alt: Mean squared error
   :srcset: /auto_examples/images/sphx_glr_plot_ucurve_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 13.991 seconds)


.. _sphx_glr_download_auto_examples_plot_ucurve.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_ucurve.ipynb <plot_ucurve.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_ucurve.py <plot_ucurve.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
